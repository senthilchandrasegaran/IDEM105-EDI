{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4906eb7b-91bf-4763-b4ff-5847f4c81de6",
   "metadata": {},
   "source": [
    "# Computational Text Analysis Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048abf89-a2a3-4a73-9a0a-464b6c83f331",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization refers to the process of dividing a text into \"tokens\": words, parts of words, phrases, punctuations, or even sentences. The most common tokenization is at the level of words. A common strategy is to \"split\" the text wherever one encounters spaces.\n",
    "\n",
    "You can type or paste in your own text in into the cell below, but make sure to keep the three double-quotes (`\"\"\"`) at the start and the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35523489-1931-4c28-82d2-36e99851792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Well the design has developed a wee bit since you saw it last time, \n",
    "the design obviously is still in exactly the same place but \n",
    "the design is extended to actually include the actual cremator facility,\n",
    "so if I can start with this particular drawing, you’ve seen a version\n",
    "of this drawing before. Basically we’re arriving in the new car park\n",
    "in this area and from the car park we’ll enter the building through a\n",
    "waiting area. This leads us to the first query I have because there was\n",
    "some discussion about whether you wanted the size of the waiting room\n",
    "increased. At the moment it’s exactly on brief, but it does look kind of\n",
    "small to my eye in relation to the size of the project.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af15935-9b9f-4ec6-a73a-5d7c82f11784",
   "metadata": {},
   "source": [
    "### Simple approach: split at the spaces\n",
    "A common strategy is to \"split\" the text wherever one encounters spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d70ce-4e3e-414a-877a-1e3ee7462c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sample_text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8c859-e96c-41b1-b140-a1f0326fee8d",
   "metadata": {},
   "source": [
    "### Preferred Approach: Use an existing library.\n",
    "Note the punctuations in the above output. They are still part of the preceding word. \n",
    "Also note contractions like `you've`. They are retained as they are. \n",
    "There are different ways to separate punctuations, contractions etc., but thankfully we can use a pre-existing library called [Natural Language Toolkit or NLTK](https://www.nltk.org/index.html#). To generate tokens, we will use a function called [word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize).\n",
    "\n",
    "Pay attention to the commented code below (lines starting with a `#`) and when applicable, uncomment them by deleting the `#`. A commented line of code is ignored by the system, and not executed.\n",
    "\n",
    "You can also toggle the commenting of any line. To do so, place your cursor on that line and hold down `Ctrl` and press `/` (for Windows/Linux) or hold down `⌘` and press `/` (for Macs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce9434-2bbf-4a9d-aa9e-6576f7de3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Uncomment this line if you don't have punkt downloaded.\n",
    "# nltk.download('punkt_tab')  \n",
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62522cda-8d2a-49ab-a0e2-e59253a76b71",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "`Import` commands (such as the ones you see above) are conventionally entered in the first cell of a Jupyter Notebook (or in the first few lines of a python program). However, we break from convention here to show when a particular library is used in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202c14b-5d59-4fbc-91cc-1092dae6fcad",
   "metadata": {},
   "source": [
    "### Removing Punctuations\n",
    "Different approaches can be used to remove punctuations. A helpful way is to use another library called [string](https://docs.python.org/3/library/string.html), which contains a list of standard punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eae962-46e9-4f9b-8316-d16a34f64c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation + '’'\n",
    "tokens_without_puncts = [word for word in tokens if word not in punctuations]\n",
    "print(tokens_without_puncts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3d663-a1a8-4155-bee6-e7c33de23ef5",
   "metadata": {},
   "source": [
    "## Counting Words\n",
    "Almost subsequent processing is about counting words at some level. A simple way to get a count of words for us is to use another library called [collections](https://docs.python.org/3/library/collections.html), and a function called [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632f4a8-57ac-4c56-9798-6c2b4d7a4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(tokens_without_puncts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa5132-3d00-4d76-997c-6b4316334eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7218f-8268-44f8-902a-879beb925ec6",
   "metadata": {},
   "source": [
    "## Letter case\n",
    "Looking at the list of non-repeating words in the sample text, we can see that capitalised letters are treated differently.\n",
    "We may or may not want this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cb938-19f3-4927-a17a-bbde22460cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_repeating_words = word_counts.keys()\n",
    "print(sorted(non_repeating_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e753e9-eabc-41df-a661-269b41cb9808",
   "metadata": {},
   "source": [
    "### Converting all text to lowercase\n",
    "\n",
    "We simply use the `.lower()` method to convert all text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b5bcbf-f6a6-4e3f-9963-9fa23575fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_tokens = [word.lower() for word in tokens_without_puncts]\n",
    "word_counts_lowercase = Counter(lowercase_tokens)\n",
    "print(word_counts_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04417bce-ee96-4298-aea6-847f041858d3",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "Note how `actual` and `actually` are treated separately. This may be necessary, or not, depending on the requirements of the analysis. If the base form of the word is to be obtained, we either have to \"stem\" the word (remove suffixes) or \"lemmatize\" the word (convert to base form).\n",
    "\n",
    "### Stemming\n",
    "\n",
    "This is the simple form where a set of rules can be used to remove inflection from the words. This may or may not work, as you can see from the below two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fac450-6004-4b22-8ac7-08335a7b640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(\"discussion :\", stemmer.stem(\"discussion\"))\n",
    "print(\"went :\", stemmer.stem(\"went\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00997f3-c8ef-4265-988a-66f4bb9627ae",
   "metadata": {},
   "source": [
    "Applying this approach to our word list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed716ed2-1f36-4940-bd76-0e25dcbfb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [stemmer.stem(word) for word in word_counts_lowercase]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8dfe9-bf88-40e9-a2e0-234031d91d60",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "This is a slightly more sophisticated version, where grammatical considerations are used to determine the base form of the word. For this, we also need to label the word with its appropriate part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a23ebf-04ad-47cb-964f-82077c0420c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment the line below after the first time you run this code.\n",
    "nltk.download('wordnet')  \n",
    "\n",
    "# comment this line after the first time you run this code.\n",
    "nltk.download('averaged_perceptron_tagger_eng')  \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatize = WordNetLemmatizer().lemmatize\n",
    "print(\"discussion :\", lemmatize(\"discussion\", pos=\"n\"))\n",
    "print(\"went :\", lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e27af-dc85-4fe0-879f-4f2b0ae3dafb",
   "metadata": {},
   "source": [
    "Applying it to our list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805bc250-3f8e-491d-a29d-832649eeae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = pos_tag(lowercase_tokens)\n",
    "lemmas_list = []\n",
    "\n",
    "for tagged_word in tagged_tokens:\n",
    "    word, pos_tag = tagged_word\n",
    "    if pos_tag.startswith(\"V\") :\n",
    "        lemma = lemmatize(word, pos=\"v\")\n",
    "    elif pos_tag.startswith(\"R\") :\n",
    "        lemma = lemmatize(word, pos=\"r\")\n",
    "    elif pos_tag.startswith(\"N\") :\n",
    "        lemma = lemmatize(word, pos=\"n\")\n",
    "    elif pos_tag.startswith(\"J\") :\n",
    "        lemma = lemmatize(word, pos=\"a\")\n",
    "    else :\n",
    "        lemma = lemmatize(word)\n",
    "    lemmas_list.append(lemma)\n",
    "\n",
    "print(lemmas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcd91b-869d-4740-adc3-0a57738c1c00",
   "metadata": {},
   "source": [
    "What difference do you see between the two lists?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b58d10-2284-4088-b2b1-0c5d13ac4e9a",
   "metadata": {},
   "source": [
    "## Extracting n-grams from text\n",
    "\n",
    "For identifying commonly-used phrases in a given text, you need to capture all possible occurrences of word sequences of the length that interests you.\n",
    "\n",
    "### Bigrams\n",
    "If the sequence is of two words, it is called a bigram. There is a function in the NLTK library, which is called `bigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7790bd-5f75-46c9-82c4-3a6e6a07eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "bigrams_from_text = list(bigrams(lowercase_tokens))\n",
    "print(bigrams_from_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b7969-1017-4f8c-9468-bc2eb87cb546",
   "metadata": {},
   "source": [
    "### Counting bigrams\n",
    "It is then a matter of simply counting the number of occurrences, similar to what we had done with words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72bcd58-3820-4ce8-bd65-b71f8e543bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = Counter(bigrams_from_text)\n",
    "print(bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3aae1-bb5c-480b-964e-b8ac438c28e1",
   "metadata": {},
   "source": [
    "### Generalizing to n-grams\n",
    "We use a similar utility called `n-grams` to generalize this idea to words of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf9daf-a998-420f-b6a6-ae29afbb66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "trigrams_from_text = list(ngrams(lowercase_tokens, 3))\n",
    "trigram_counts = Counter(trigrams_from_text)\n",
    "print(trigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7056b7-4c7d-4e9d-8c86-16a49a8391e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = ['draw', 'drawing', 'drew', 'drawer', 'drawn']\n",
    "lemmas_test = [lemmatize(w, 'v') for w in test_words]\n",
    "print(lemmas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04507d8-4ca4-4867-bb90-8fcf92fd1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems_test = [stemmer.stem(w) for w in test_words]\n",
    "print(stems_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2396ff-825c-40d2-bb71-c785037fe5f8",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "Some words can be seen to contain less \"information\" than others. The commonly-occurring words are called \"stop words\". There is a library that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88539f08-4f05-4db3-9cf2-a21ed3e74ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# comment this line after the first time you run this code.\n",
    "nltk.download('stopwords') \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b9070-5c68-46aa-8920-e5aebee5b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcase_tokens_ns = [word for word in lowercase_tokens if not word in stop_words]\n",
    "word_counts_lcase_ns = Counter(lcase_tokens_ns)\n",
    "print(word_counts_lcase_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191194c0-773e-4c5a-8a82-c0368f034382",
   "metadata": {},
   "source": [
    "What difference do you see in the word counts?\n",
    "\n",
    "Try playing with other inputs in the notebook above, and when you are comfortable with most of the commands, move on to the next notebook: making sequential word clouds from a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff25f9-230b-42fa-99d5-a2ac26862fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
