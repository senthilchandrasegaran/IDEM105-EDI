{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c120e926-8438-475c-8063-e3c1e46b8d8e",
   "metadata": {},
   "source": [
    "# Create a \"sequence\" with word clouds\n",
    "\n",
    "A simple word cloud of an entire text does not often convey the changes that occur over the course of a conversation or a narrative.\n",
    "However, by splitting the text into segments and generating word clouds for each, one can get a better sense of these changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a7ceb-ff48-4d79-87d4-cdf6360f2fae",
   "metadata": {},
   "source": [
    "## Load a text file\n",
    "Open a standard text file that you might have saved locally. Update the file path below with one that links to your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73919e9-beee-45d1-8e3f-6245afa8822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/gutenberg/carroll-alice.txt', 'r') as fo:\n",
    "    text = fo.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260586f7-ddfb-4a61-9018-a073ce5fc934",
   "metadata": {},
   "source": [
    "If you downloaded the book from Project Gutenberg, you might have text that you don't need. Use the code below to figure out where the actual text starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978b464-5f96-43fe-b301-c4daf23e4782",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(text[0:30]):\n",
    "    print(\"%2d : %s\" % (index, line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b695baff-1ba2-47d6-9e7f-14b1e436137a",
   "metadata": {},
   "source": [
    "----\n",
    "Note down the line number where the actual text begins and get rid of the text until that line.\n",
    "\n",
    "You might also see a number of carriage returns (`\\n`) and empty lines, which you can also remove if you prefer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1774dbc-d9b0-4848-a4be-fb5587900ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Get rid of lines containing table of contents\n",
    "text=text[23:] \n",
    "\n",
    "# remove all carriage returns within lines.\n",
    "text = [line.replace('\\n', '') for line in text] \n",
    "\n",
    "# remove all empty lines\n",
    "text = [line for line in text if len(line) > 0]  \n",
    "\n",
    "pprint.pp(text[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5906c-6c15-40fe-ac89-be6ad7391d89",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenize the text\n",
    "\n",
    "The above output should look good (no empty lines, no table of contents etc.).\n",
    "Now we can collapse the text into one long string, and start separating the words.\n",
    "We use the [Punkt tokenizer](https://www.nltk.org/api/nltk.tokenize.punkt.html) to separate text into tokens.\n",
    "This tokenizer uses an unsupervised algorithm (trained on large amounts of plain text) to build a model for abbreviation words, collocations (words that often go together), and words that start sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbca712-85ee-400e-9543-ed803c75cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Uncomment the below line the first time you run this code.\n",
    "# nltk.download('punkt_tab')  \n",
    "\n",
    "text_str = ' '.join(text)\n",
    "tokens = word_tokenize(text_str)\n",
    "print(\"%d words in text, including punctuations\" % (len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61152d0c-e871-4374-843c-42715da74457",
   "metadata": {},
   "source": [
    "### Remove punctuations\n",
    "We have not made any effort to get rid of punctuations, so let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5399f5-eb01-4ff9-8f37-a875298f0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all punctuations\n",
    "import string\n",
    "\n",
    "# Add punctuations you might spot in the text\n",
    "# that may not be in the list of standard punctuations.\n",
    "punctuations = string.punctuation + 'â€™' \n",
    "\n",
    "tokens_without_puncts = [w for w in tokens if w not in punctuations]\n",
    "\n",
    "print(\"%d words in text, excluding punctuations\" \n",
    "      % (len(tokens_without_puncts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787279c-5a2b-456d-a420-ddb2058e3dfa",
   "metadata": {},
   "source": [
    "## Segment the text for the sequential word cloud\n",
    "Split the text into a number of segments, say N. \n",
    "We want the value of \"N\" to be adjustable, so we use it as a variable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e7e0f-81ba-41c0-ba85-e0865a2951ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text(tokens_list, num_segments, remaining_spaces=''):\n",
    "    # Identify the closest number of tokens to place in each segment.\n",
    "    segment_size = int(round(len(tokens_list)/num_segments, 0))\n",
    "    # The closest number might be slightly less than needed to cover\n",
    "    # the entire text, especially if the above operation results in\n",
    "    # rounding down the value. To make up for this case, increase the\n",
    "    # number by one in each segment.\n",
    "    if segment_size * num_segments < len(tokens_list) :\n",
    "        segment_size += 1\n",
    "\n",
    "    # allocate the text sequentially to each segment\n",
    "    list_of_segments = []\n",
    "    for ind in range(num_segments):\n",
    "        start_of_segment = ind * segment_size\n",
    "        end_of_segment = (ind+1) * segment_size - 1\n",
    "        segment_tokens = tokens_list[start_of_segment : end_of_segment]\n",
    "        list_of_segments.append(segment_tokens)\n",
    "    return segment_size, list_of_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfdb01c-3777-4679-b40e-1940caa1ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 10\n",
    "words_per_wc, wc_token_lists = segment_text(tokens_without_puncts,\n",
    "                                            num_segments)\n",
    "print(\"%d words segmented into %d segments of %d words each\" \n",
    "      % (len(tokens_without_puncts), num_segments, words_per_wc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f4771-2813-4a3a-a958-9472fa52d23e",
   "metadata": {},
   "source": [
    "### Plot word cloud for each segment\n",
    "We use the existing [WordCloud](https://github.com/amueller/word_cloud) library for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a1fb0-aee4-4a4c-b7fe-72ddbaa1d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, dpi=300)\n",
    "widths = [1] * num_segments\n",
    "\n",
    "color_func = lambda *args, **kwargs: 'black'\n",
    "\n",
    "# We make one figure to contain all N subfigures :\n",
    "spec = fig.add_gridspec(ncols=num_segments,\n",
    "                        width_ratios=widths,\n",
    "                        wspace=0.0, hspace=0.0)\n",
    "\n",
    "# Then, we iterate over our segmented lists of words, \n",
    "# and generate word clouds using the library we imported.\n",
    "\n",
    "for ind, words_list in enumerate(wc_token_lists):\n",
    "    words_str = ' '.join(words_list)\n",
    "    ax = fig.add_subplot(spec[ind])\n",
    "    wc = WordCloud(width=500, height=800,\n",
    "                   background_color=\"white\",\n",
    "                   color_func=color_func,\n",
    "                   max_words=100,\n",
    "                   stopwords=STOPWORDS)\n",
    "    wc.generate_from_text(words_str)\n",
    "    ax.imshow(wc)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Save the figure.\n",
    "plt.savefig('./plots/sequence_wordcloud.pdf', bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35f8e2-4c13-4abd-a5d0-5bd15123c613",
   "metadata": {},
   "source": [
    "If you are interested in the use of `lambda` in the above code, read on.\n",
    "\n",
    "Lambda is a concept in functional programming that lets us use functions as variables to pass to other functions.\n",
    "\n",
    "We use a lambda function here because of how color is specified\n",
    "in the WordCloud library (see [this example](https://amueller.github.io/word_cloud/auto_examples/a_new_hope.html)).\n",
    "We set it up to pass a variable number of arguments (`*args`) e.g. (1, 2) or (1, 2, 3, ...)\n",
    "and keyword arguments (`**kwargs`) e.g. (a=1, b=1) or (a=1, b=1, c=2, ...) and return **'black'** for all items.\n",
    "\n",
    "Lambda functions are not essential for this workshop, but it is an interesting concept. If you are interested, read [this article](https://realpython.com/python-lambda/) for more detailed information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2a889-835d-4bdd-ba91-346dc9ef94ab",
   "metadata": {},
   "source": [
    "### Keyword in context analysis\n",
    "Are you interested in any particular word that caught your attention in the text or the word clouds?\n",
    "You can use the below code to examine the context in which it appears in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28aa7b1-1624-44bf-ac65-d26493254f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "textList = Text(tokens_without_puncts)\n",
    "textList.concordance('gryphon', width=85, lines=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cc664-bb60-449f-8527-cdb76a6dc1cd",
   "metadata": {},
   "source": [
    "---\n",
    "Try the option below if you are interested in a sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e493c54-807c-4b76-80cf-e69e7300b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "textList.concordance(['Mock', 'Turtle'], width=85, lines=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77685efc-4492-492a-9a33-a5ef3bdc85c0",
   "metadata": {},
   "source": [
    "## Exercise for you!\n",
    "Instead of splitting the text uniformly, can you draw a similar word cloud for each chapter?\n",
    "\n",
    "Additional challenges:\n",
    "\n",
    "  - [ ] Scale each word cloud segment to match the size of the chapter it represents. Longer chapters get bigger word clouds and vice versa.\n",
    "  - [ ] Analyse each segment for its tone, sentiment, or a related LIWC-like psycholinguistic aspect. Can you draw the word cloud sequence as a \"bar chart\" with the heights representing a that value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890e116-3733-4b78-ab03-3d52f496ee38",
   "metadata": {},
   "source": [
    "Here is a bit of code to start you off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dcafbd-6a2d-403b-a03a-2d149b3869cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = [text for text in text_str.strip().split(\"CHAPTER\") if len(text) > 0]\n",
    "len(chapters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
